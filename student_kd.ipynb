{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dependencies, Model and Data"
      ],
      "metadata": {
        "id": "QHcnhu3F6EmG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg4FDkhYUDoi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lvl6svMwVmdu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import wandb\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForMultipleChoice, AutoModelForMaskedLM, BertModel, BertConfig\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import load_dataset, load_from_disk, DatasetDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "import random\n",
        "import math\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwHcsLHjX9YZ"
      },
      "outputs": [],
      "source": [
        "# Load the PubMed Abstracts\n",
        "\n",
        "train_dataset = load_dataset(\"ywchoi/pubmed_abstract_1\", split='train')\n",
        "val_dataset = load_dataset(\"ywchoi/pubmed_abstract_1\", split='validation')\n",
        "\n",
        "# Susbet data (for experimentation)\n",
        "\n",
        "train_dataset = train_dataset.select(( i for i in range(int(len(train_dataset)/1))))\n",
        "val_dataset = train_dataset.select(( i for i in range(int(len(train_dataset)/1))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GPuyPakf0_y"
      },
      "outputs": [],
      "source": [
        "def initializeStudent(save_path):\n",
        "  \n",
        "  ''' Initiliazes student model as a subset of layers from the teacher. Adapted from\n",
        "      https://github.com/nlpie-research/Compact-Biomedical-Transformers/blob/main/DistilBioBERT-Distillation.py '''\n",
        "\n",
        "  bertModel = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
        "\n",
        "  distilBertConfig = bertModel.config.to_dict()\n",
        "  distilBertConfig[\"num_hidden_layers\"] //= 2\n",
        "\n",
        "  distillationModel = BertModel(config= BertConfig.from_dict(distilBertConfig))\n",
        "  distillationModel.embeddings = bertModel.embeddings\n",
        "\n",
        "  for index,layer in enumerate(distillationModel.encoder.layer):\n",
        "    distillationModel.encoder.layer[index] = bertModel.encoder.layer[2*index]\n",
        "\n",
        "  distillationModel.save_pretrained(save_path)\n",
        "\n",
        "  return save_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-5WxnKSTCDQ"
      },
      "outputs": [],
      "source": [
        "# Initialize student model and load teacher model\n",
        "\n",
        "#student_model = AutoModelForMaskedLM.from_pretrained('distilbert-base-cased')\n",
        "student_model = AutoModelForMaskedLM.from_pretrained(initializeStudent('initialized_model/'))\n",
        "student_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
        "\n",
        "teacher_model = AutoModelForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View model sizes \n",
        "\n",
        "print(student_model.num_parameters())\n",
        "print(assistant_model.num_parameters())\n",
        "print(teacher_model.num_parameters())"
      ],
      "metadata": {
        "id": "7Mjjxh-i8Stw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB_i-iVAdmH4"
      },
      "outputs": [],
      "source": [
        "for param in teacher_model.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsxihiE0t9AQ"
      },
      "source": [
        "## Continued student pretraining w/ KD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjpZi3bwowWo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from transformers.modeling_outputs import MaskedLMOutput\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRdzr-LTnNP4"
      },
      "outputs": [],
      "source": [
        "class DistillationWrapper(nn.Module):\n",
        "\n",
        "  ''' Distillation code. Adapted from https://github.com/nlpie-research/Compact-Biomedical-Transformers/blob/main/DistilBioBERT-Distillation.py '''\n",
        "\n",
        "  def __init__(self,\n",
        "               student, \n",
        "               teacher, \n",
        "               temperature=2.0, \n",
        "               alpha_ce=5.0, \n",
        "               alpha_mlm=2.0, \n",
        "               alpha_cos=1.0):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.student = student\n",
        "    self.teacher = teacher\n",
        "\n",
        "    self.temperature = temperature\n",
        "    self.vocab_size = self.teacher.config.vocab_size\n",
        "    self.dim = self.teacher.config.hidden_size\n",
        "\n",
        "    self.restrict_ce_to_mask = True\n",
        "\n",
        "    self.alpha_ce = alpha_ce\n",
        "    self.alpha_mlm = alpha_mlm\n",
        "    self.alpha_cos = alpha_cos\n",
        "\n",
        "    self.ce_loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    self.lm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    self.cosine_loss_fct = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
        "\n",
        "  def forward(self, \n",
        "              input_ids, \n",
        "              attention_mask,\n",
        "              labels=None,\n",
        "              **kargs):\n",
        "\n",
        "    student_outputs = self.student(input_ids=input_ids,\n",
        "                                   attention_mask=attention_mask,\n",
        "                                   labels=labels,\n",
        "                                   output_hidden_states=True,\n",
        "                                   **kargs)   \n",
        "    \n",
        "    s_logits, s_hidden_states = student_outputs[\"logits\"], student_outputs[\"hidden_states\"]\n",
        "\n",
        "    loss = None\n",
        "\n",
        "    if labels != None:\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        teacher_outputs = self.teacher(input_ids=input_ids,\n",
        "                                       attention_mask=attention_mask,\n",
        "                                       output_hidden_states=True,\n",
        "                                       **kargs)\n",
        "\n",
        "      t_logits, t_hidden_states = teacher_outputs[\"logits\"], teacher_outputs[\"hidden_states\"]    \n",
        "\n",
        "\n",
        "      if self.restrict_ce_to_mask:\n",
        "        mask = (labels > -1).unsqueeze(-1).expand_as(s_logits).bool()\n",
        "      else:\n",
        "        mask = attention_mask.unsqueeze(-1).expand_as(s_logits).bool()\n",
        "\n",
        "      s_logits_slct = torch.masked_select(s_logits, mask)  \n",
        "      s_logits_slct = s_logits_slct.view(-1, s_logits.size(-1))  \n",
        "      t_logits_slct = torch.masked_select(t_logits, mask)  \n",
        "      t_logits_slct = t_logits_slct.view(-1, s_logits.size(-1)) \n",
        "      assert t_logits_slct.size() == s_logits_slct.size()\n",
        "      \n",
        "      loss_mlm = student_outputs.loss\n",
        "\n",
        "      loss_ce = (\n",
        "          self.ce_loss_fct(\n",
        "              nn.functional.log_softmax(s_logits_slct / self.temperature, dim=-1),\n",
        "              nn.functional.softmax(t_logits_slct / self.temperature, dim=-1),\n",
        "          )\n",
        "          * (self.temperature) ** 2\n",
        "      )\n",
        "\n",
        "      loss = (self.alpha_mlm * loss_mlm) + (self.alpha_ce * loss_ce)\n",
        "\n",
        "      if self.alpha_cos > 0.0:\n",
        "          s_hidden_states = s_hidden_states[-1]  # (bs, seq_length, dim)\n",
        "          t_hidden_states = t_hidden_states[-1]  # (bs, seq_length, dim)\n",
        "          mask = attention_mask.unsqueeze(-1).expand_as(s_hidden_states).bool()  # (bs, seq_length, dim)\n",
        "          assert s_hidden_states.size() == t_hidden_states.size()\n",
        "          dim = s_hidden_states.size(-1)\n",
        "\n",
        "          s_hidden_states_slct = torch.masked_select(s_hidden_states, mask)  # (bs * seq_length * dim)\n",
        "          s_hidden_states_slct = s_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n",
        "          t_hidden_states_slct = torch.masked_select(t_hidden_states, mask)  # (bs * seq_length * dim)\n",
        "          t_hidden_states_slct = t_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n",
        "\n",
        "          target = s_hidden_states_slct.new(s_hidden_states_slct.size(0)).fill_(1)  # (bs * seq_length,)\n",
        "          loss_cos = self.cosine_loss_fct(s_hidden_states_slct, t_hidden_states_slct, target)\n",
        "          loss += (self.alpha_cos * loss_cos)\n",
        "\n",
        "\n",
        "    return MaskedLMOutput(\n",
        "        loss=loss,\n",
        "        logits=student_outputs.logits,\n",
        "        hidden_states=student_outputs.hidden_states,\n",
        "        attentions=student_outputs.attentions,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjhVDy75qWeR"
      },
      "outputs": [],
      "source": [
        "model = DistillationWrapper(student=student_model, teacher=teacher_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BSL80e8r8Rt"
      },
      "outputs": [],
      "source": [
        "# Tokenize and collate\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return student_tokenizer(examples[\"text\"], padding=\"max_length\", max_length=512, truncation=True)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=student_tokenizer, mlm=True, mlm_probability=0.15, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "zjexfI2e6bcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJjt9FrxrJGC"
      },
      "outputs": [],
      "source": [
        "## Continued pretraining setup\n",
        "\n",
        "savePath = \"learned_student_kd_2/\"\n",
        "\n",
        "trainingArguments = TrainingArguments(\n",
        "    output_dir= savePath + \"checkpoints\",\n",
        "    #evaluation_strategy=\"steps\",\n",
        "    #eval_steps=200,\n",
        "    logging_steps=1000,\n",
        "    overwrite_output_dir=True,\n",
        "    save_steps=250,\n",
        "    num_train_epochs=0.5, \n",
        "    learning_rate=5e-4,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=5000,\n",
        "    per_device_train_batch_size=24, \n",
        "    weight_decay=0.0,\n",
        "    save_total_limit=5,\n",
        "    remove_unused_columns=True,\n",
        "    report_to=\"wandb\"\n",
        ") \n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=trainingArguments,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    #callbacks=[ts.ProgressCallback(), CustomCallback()],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPhN7fUio1Hv"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "\n",
        "trainer.save_model(\"/content/learned_student_kd_trainersave/\")\n",
        "model.student.save_pretrained(\"/content/learned_student_kd/\")\n",
        "!zip -r /content/learned_student_kd.zip /content/learned_student_kd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model model to Google Drive\n",
        "\n",
        "import shutil\n",
        "colab_link = \"/content/learned_student_kd.zip\"\n",
        "gdrive_link = \"/content/drive/MyDrive/CLS/KD-CL-SR/\"\n",
        "shutil.copy(colab_link, gdrive_link)"
      ],
      "metadata": {
        "id": "xaDLTe6r9fRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfeDfpmelJKd"
      },
      "source": [
        "## Finetune and evaluate on student BioASQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG3LmdmS9x7B"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOeJ4IQO9zrJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForQuestionAnswering, AutoModelForSequenceClassification, AutoModelForMultipleChoice\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzD38hixl8WH"
      },
      "outputs": [],
      "source": [
        "# Load learned model\n",
        "\n",
        "learned_student = AutoModelForMultipleChoice.from_pretrained(\"/content/learned_student_kd\")\n",
        "learned_tokenizer = student_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlK3la9Dlqoa"
      },
      "outputs": [],
      "source": [
        "def stratify(dataset, yes_max, no_max):\n",
        "  \n",
        "  \" Simple class balancing function w/ shuffling\"\n",
        "\n",
        "  yes_count = 0\n",
        "  no_count = 0\n",
        "\n",
        "  exclude_id = []\n",
        "\n",
        "  for i in range(len(dataset)):\n",
        "\n",
        "    if (dataset[i]['answers'] == \"yes\"): \n",
        "      yes_count+=1\n",
        "      if yes_count > yes_max:\n",
        "        exclude_id.append(i)\n",
        "    \n",
        "    if (dataset[i]['answers'] == \"no\"):   \n",
        "      no_count+=1\n",
        "      if no_count > no_max:\n",
        "        exclude_id.append(i)\n",
        "\n",
        "  dataset = dataset.select(\n",
        "      (\n",
        "          i for i in range(len(dataset)) \n",
        "          if i not in set(exclude_id)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  return dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKH5UOaWlz_Y"
      },
      "outputs": [],
      "source": [
        "# Load the labeled BioASQ dataset\n",
        "\n",
        "dataset = load_dataset(\"reginaboateng/Bioasq7b\")['train']\n",
        "\n",
        "# Balance classes\n",
        "\n",
        "dataset_balanced = stratify(dataset, 883, 883)\n",
        "train_dataset = dataset_balanced.select(( i for i in range(0, 1500)))\n",
        "val_dataset = dataset_balanced.select(( i for i in range(1500, 1600)))\n",
        "test_dataset = dataset_balanced.select(( i for i in range(1600, 1766)))\n",
        "\n",
        "# Add numeric label column for all datasets\n",
        "\n",
        "d = {'yes' : 0, 'no': 1}\n",
        "new_column = [d[fd] for fd in train_dataset['answers']] \n",
        "train_dataset = train_dataset.add_column(\"label\", new_column)\n",
        "new_column = [d[fd] for fd in val_dataset['answers']] \n",
        "val_dataset = val_dataset.add_column(\"label\", new_column)\n",
        "new_column = [d[fd] for fd in test_dataset['answers']] \n",
        "test_dataset = test_dataset.add_column(\"label\", new_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UhFGmgesoXm"
      },
      "outputs": [],
      "source": [
        "# Tokenize the data \n",
        "\n",
        "def preprocess(example):\n",
        "\n",
        "  batch_size = 2\n",
        "  answers = [\"yes\", \"no\"]\n",
        "  context = [[c] * len(answers) for c in example[\"context\"]]\n",
        "  question_headers = example[\"question\"]\n",
        "  \n",
        "  question_answer = [\n",
        "      [f\"{header} {a}\" for a in answers] for i, header in enumerate(question_headers)\n",
        "  ]\n",
        "\n",
        "  context = sum(context, [])\n",
        "  question_answer = sum(question_answer, [])\n",
        "  \n",
        "  tokenized_examples = learned_tokenizer(context, question_answer, truncation='only_first', max_length=512)\n",
        "  \n",
        "  return {k: [v[i : i + batch_size] for i in range(0, len(v), len(answers))] for k, v in tokenized_examples.items()}\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(preprocess, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(preprocess, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkI0_yBNtPfr"
      },
      "outputs": [],
      "source": [
        "# Load data collator\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from typing import Optional, Union\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "    \n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs for multiple choice received. Adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features):\n",
        "      \n",
        "      label_name = \"label\" if \"label\" in features[0].keys() else \"label\"\n",
        "      labels = [feature.pop(label_name) for feature in features]\n",
        "      batch_size = len(features)\n",
        "      num_choices = len(features[0][\"input_ids\"])\n",
        "      \n",
        "      flattened_features = [\n",
        "          [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
        "   \n",
        "      flattened_features = sum(flattened_features, [])\n",
        "\n",
        "      batch = self.tokenizer.pad(\n",
        "          flattened_features,\n",
        "          padding=self.padding,\n",
        "          max_length=self.max_length,\n",
        "          pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "\n",
        "      batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "      batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "      #print('batch input_id size: ', batch['input_ids'].shape)\n",
        "      #print('batch token_type_id size: ', batch['token_type_ids'].shape)\n",
        "      #print('batch attention_mask size: ', batch['attention_mask'].shape)\n",
        "      #print('batch labels: ', batch['labels'])\n",
        "      #print('labels size: ', len(labels))\n",
        "\n",
        "      #print('\\nBatch: ', batch)\n",
        "\n",
        "      return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZGKSVWGthhR"
      },
      "outputs": [],
      "source": [
        "# Load evaluation metrics\n",
        "\n",
        "from datasets import load_metric\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uce51IWtsMg"
      },
      "source": [
        "### Hyperparameter Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXsleqRitq5q"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oIpCQS_tr6P"
      },
      "outputs": [],
      "source": [
        "# W&B hyperparameter specification\n",
        "\n",
        "# method\n",
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "}\n",
        "\n",
        "# hyperparameters\n",
        "parameters_dict = {\n",
        "    'epochs': {\n",
        "        'values': [1, 2]\n",
        "        },\n",
        "    'batch_size': {\n",
        "        'values': [4, 8, 12]\n",
        "        },\n",
        "    'learning_rate': {\n",
        "        'distribution': 'log_uniform_values',\n",
        "        'min': 1e-6,\n",
        "        'max': 1e-3\n",
        "    },\n",
        "    'weight_decay': {\n",
        "        'values': [0.05, 0.1, 0.15]\n",
        "    },\n",
        "}\n",
        "\n",
        "metric = {\n",
        "    'name' : 'loss',\n",
        "    'goal' : 'minimize'\n",
        "}\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project='learned-student-kd-bioasq-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx45bdqtuTUV"
      },
      "outputs": [],
      "source": [
        "# W&B trainer setup\n",
        "\n",
        "preds = []\n",
        "test_accs = []\n",
        "\n",
        "def fine_tune(config=None):\n",
        "\n",
        "  with wandb.init(config=config):\n",
        "    # set sweep configuration\n",
        "    config = wandb.config\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "      output_dir=\"/content/wandb/outputs\", \n",
        "      evaluation_strategy=\"steps\",\n",
        "      save_strategy=\"steps\",\n",
        "      eval_steps=20,\n",
        "      load_best_model_at_end=True,\n",
        "      #learning_rate=5e-4,\n",
        "      learning_rate=config.learning_rate,\n",
        "      per_device_train_batch_size=config.batch_size,\n",
        "      per_device_eval_batch_size=config.batch_size,\n",
        "      num_train_epochs=config.epochs,\n",
        "      weight_decay=config.weight_decay,\n",
        "      logging_steps=10,\n",
        "      push_to_hub=False,\n",
        "      report_to=\"wandb\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=learned_student,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train_dataset,\n",
        "        eval_dataset=tokenized_val_dataset,\n",
        "        tokenizer=learned_tokenizer,\n",
        "        data_collator=DataCollatorForMultipleChoice(tokenizer=learned_tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "wandb.agent(sweep_id, fine_tune, count=20) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNxhuOV8E1Cy"
      },
      "outputs": [],
      "source": [
        "test_results = trainer.predict(test_dataset=tokenized_test_dataset)\n",
        "test_results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}